{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0017a479-b454-4843-9c47-3c4fd0d298b2",
   "metadata": {},
   "source": [
    "# CLIP-DINOiser visualization demo üñºÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "id": "cc27d12a1279e4cc",
   "metadata": {},
   "source": [
    "from hydra.core.global_hydra import GlobalHydra\n",
    "import os\n",
    "from models.builder import build_model\n",
    "from helpers.visualization import mask2rgb\n",
    "from segmentation.datasets import PascalVOCDataset\n",
    "from hydra import compose, initialize\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms as T\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "GlobalHydra.instance().clear()\n",
    "initialize(config_path=\"configs\", version_base=None)\n",
    "\n",
    "def visualize_per_image(file_path, support_files, palette, model, class_names):\n",
    "    # Assert that the main image exists\n",
    "    assert os.path.isfile(file_path), f\"No such file: {file_path}\"\n",
    "    \n",
    "    # print(f\"Dataset classes: {dataset_classes}\")\n",
    "\n",
    "    # Open and preprocess the main image\n",
    "    img = Image.open(file_path).convert('RGB')\n",
    "    img_tens = T.PILToTensor()(img).unsqueeze(0).to(device) / 255.\n",
    "\n",
    "    # Load and preprocess support images\n",
    "    support_images = []\n",
    "    for support_file in support_files:\n",
    "        assert os.path.isfile(support_file), f\"No such file: {support_file}\"\n",
    "        support_img = Image.open(support_file).convert('RGB')\n",
    "\n",
    "        # Resize the support images to match the size of the main image\n",
    "        support_img_resized = support_img.resize(img.size, Image.BILINEAR)\n",
    "\n",
    "        # Convert to tensor and normalize\n",
    "        support_img_tens = T.PILToTensor()(support_img_resized).unsqueeze(0).to(device) / 255.\n",
    "        support_images.append(support_img_tens)\n",
    "    \n",
    "    # Stack the support images into a batch\n",
    "    support_images = torch.cat(support_images, dim=0).to(device)  \n",
    "    # for spt in support_images:\n",
    "    #     print(spt.shape)\n",
    "    \n",
    "    # Get the original height and width of the image\n",
    "    h, w = img_tens.shape[-2:]\n",
    "    merged = torch.cat((img_tens, support_images), dim=0)\n",
    "    # print(f'merged: {merged.shape}')\n",
    "    \n",
    "    # Run the model for segmentation using both the main image and the support images\n",
    "    output = model(merged).cpu()  # Ensure your model can accept both inputs\n",
    "    output = F.interpolate(output, scale_factor=model.vit_patch_size, mode=\"bilinear\", align_corners=False)[..., :h, :w]\n",
    "    \n",
    "    # Visualizza le probabilit√† per ogni classe\n",
    "    # output[0] contiene le probabilit√† per ciascun pixel e ciascuna classe\n",
    "    # output[0].shape sar√† [C, H, W], dove C √® il numero di classi\n",
    "    \n",
    "    # Itera su tutte le classi per stampare le probabilit√†\n",
    "    # C, H, W = output[0].shape\n",
    "    # for class_idx in range(C):\n",
    "    #     class_probs = output[0][class_idx]\n",
    "    #     print(f\"Classe {class_idx}:\")\n",
    "    #     print(f\"Probabilit√† media per classe {class_idx}: {class_probs.mean().item()}\")\n",
    "    #     print(\"-\" * 50)\n",
    "    \n",
    "    output = output[0].argmax(dim=0)  # Get the most likely class for each pixel\n",
    "    \n",
    "    # Convert the output to an RGB mask using the provided palette\n",
    "    mask = mask2rgb(output, palette)\n",
    "\n",
    "    # Extract unique class indices from the output mask and map to the palette\n",
    "    detected_classes = np.unique(output).tolist()\n",
    "    # print(f\"Detected classes (indices): {detected_classes}\")\n",
    "\n",
    "    # Filter out the classes that were not detected and print their names\n",
    "    detected_class_names = [class_names[idx] for idx in detected_classes if idx < len(class_names)]\n",
    "    # print(f\"Detected class names: {detected_class_names}\")\n",
    "\n",
    "    # Create the visualization: segmented mask and original image\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "    \n",
    "    # Blending the original image with the mask for visualization\n",
    "    alpha = 0.5\n",
    "    blend = (alpha) * np.array(img) / 255. + (1 - alpha) * mask / 255.\n",
    "    ax[0].imshow(blend)\n",
    "    ax[0].axis('off')\n",
    "    \n",
    "    ax[1].imshow(mask)\n",
    "    ax[1].axis('off')\n",
    "    \n",
    "    # Visualization of class colors along with support image file names as labels\n",
    "    class_colors = np.array([palette[class_idx] for class_idx in detected_classes if class_idx < len(palette)])\n",
    "    plt.figure(figsize=(6, 1))\n",
    "    plt.imshow(class_colors.reshape(1, -1, 3))\n",
    "    plt.xticks(np.arange(len(detected_class_names)), detected_class_names, rotation=45)\n",
    "    plt.yticks([])\n",
    "\n",
    "    return mask, fig, img\n",
    "\n",
    "check_path = './checkpoints/last.pt'\n",
    "check = torch.load(check_path, map_location='cpu')\n",
    "dinoclip_cfg = \"clip_dinoiser.yaml\"\n",
    "cfg = compose(config_name=dinoclip_cfg)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = build_model(cfg.model, class_names=PascalVOCDataset.CLASSES).to(device)\n",
    "model.clip_backbone.decode_head.use_templates = False  # switching off the imagenet templates for fast inference\n",
    "model.load_state_dict(check['model_state_dict'], strict=False)\n",
    "model = model.eval()\n",
    "# TEST WITH TWO SUPPORT IMAGES\n",
    "file = 'assets/airplane.jpg'\n",
    "support_files = ['assets/air2.jpg']  # Two support images\n",
    "\n",
    "PALETTE = [(0, 0, 0), (156, 143, 189), (79, 158, 101)]\n",
    "\n",
    "# Run segmentation with two support images, no text prompts required\n",
    "model.apply_found = True  # assuming this flag is still relevant for your setup\n",
    "\n",
    "# Lista dei nomi delle classi (questo esempio √® per 3 classi)\n",
    "class_names = ['background', 'aeroplane', 'bicycle']\n",
    "# Run segmentation\n",
    "mask, ticks, img = visualize_per_image(file, support_files, PALETTE, model, class_names)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation",
   "id": "7e7a452390628c17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T14:30:27.667962Z",
     "start_time": "2024-11-08T14:26:58.663935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from datasets_custom import PascalVOCDataset # Import the PascalVOCDataset class\n",
    "\n",
    "dinoclip_cfg = \"clip_dinoiser.yaml\"\n",
    "cfg = compose(config_name=dinoclip_cfg)\n",
    "\n",
    "def calculate_confusion_matrix(pred, target, num_classes):\n",
    "    \"\"\"\n",
    "    Calculate the confusion matrix for a single batch.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): Predicted segmentation map.\n",
    "        target (Tensor): Ground truth segmentation map.\n",
    "        num_classes (int): Number of classes.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Confusion matrix for the batch.\n",
    "    \"\"\"\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1).long()  # Convert target to integers\n",
    "    mask = (target >= 0) & (target < num_classes)\n",
    "    hist = np.bincount(\n",
    "        num_classes * target[mask].cpu().numpy() + pred[mask].cpu().numpy(),\n",
    "        minlength=num_classes ** 2\n",
    "    ).reshape(num_classes, num_classes)\n",
    "    return hist\n",
    "\n",
    "def evaluate_model_with_support(model, images_per_class, num_classes):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the given images per class and calculate the mean IoU.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to evaluate.\n",
    "        images_per_class (dict): Dictionary of images for each class.\n",
    "        num_classes (int): Number of classes.\n",
    "\n",
    "    Returns:\n",
    "        float: Mean IoU across all images.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    confusion_matrix = np.zeros((num_classes, num_classes))\n",
    "    with torch.no_grad():\n",
    "        for class_name, images in images_per_class.items():\n",
    "            if class_name != 'aeroplane' or len(images) < 3:\n",
    "                continue  # Skip if not aeroplane or fewer than 3 images\n",
    "\n",
    "            for i in range(0, len(images) - 2, 3):\n",
    "                main_image = images[i]\n",
    "                support_images = images[i+1:i+3]\n",
    "\n",
    "                # Resize images to the same size\n",
    "                main_image = F.interpolate(main_image, size=(512, 512), mode='bilinear', align_corners=False)\n",
    "                support_images = [F.interpolate(img, size=(512, 512), mode='bilinear', align_corners=False) for img in support_images]\n",
    "\n",
    "                # Preprocess images\n",
    "                main_image = main_image.to(device)\n",
    "                support_images = torch.cat(support_images, dim=0).to(device)\n",
    "\n",
    "                # Concatenate main image and support images\n",
    "                merged = torch.cat((main_image, support_images), dim=0)\n",
    "\n",
    "                # Forward pass through the model\n",
    "                outputs = model(merged)\n",
    "                outputs = F.interpolate(outputs, size=main_image.shape[-2:], mode='bilinear', align_corners=False)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                confusion_matrix += calculate_confusion_matrix(preds, main_image, num_classes)\n",
    "                print(f'Tested on class {class_name}, images {i+1} to {i+3}')\n",
    "\n",
    "    # Calculate mIoU from the confusion matrix\n",
    "    intersection = np.diag(confusion_matrix)\n",
    "    union = np.sum(confusion_matrix, axis=0) + np.sum(confusion_matrix, axis=1) - intersection\n",
    "    iou = intersection / np.maximum(union, 1)\n",
    "    miou = np.nanmean(iou)\n",
    "    return miou\n",
    "\n",
    "def denormalize(img, mean, std):\n",
    "    \"\"\"\n",
    "    Denormalize an image tensor.\n",
    "\n",
    "    Args:\n",
    "        img (Tensor): Normalized image tensor.\n",
    "        mean (list): Mean values for each channel.\n",
    "        std (list): Standard deviation values for each channel.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Denormalized image tensor.\n",
    "    \"\"\"\n",
    "    mean = torch.tensor(mean).view(3, 1, 1)\n",
    "    std = torch.tensor(std).view(3, 1, 1)\n",
    "    img = img * std + mean\n",
    "    return img\n",
    "\n",
    "def show_aeroplane_images(dataset, class_name='aeroplane'):\n",
    "    \"\"\"\n",
    "    Show all images of the specified class from the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (PascalVOCDataset): The dataset to search.\n",
    "        class_name (str): The class name to filter images by.\n",
    "    \"\"\"\n",
    "    # Create a DataLoader for the dataset\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "    # Get the index of the specified class\n",
    "    classes = PascalVOCDataset.CLASSES\n",
    "    class_idx = classes.index(class_name)\n",
    "\n",
    "    # Mean and std used for normalization\n",
    "    mean = [123.675, 116.28, 103.53]\n",
    "    std = [58.395, 57.12, 57.375]\n",
    "    count = 0\n",
    "    images_per_class = {class_name: []}\n",
    "\n",
    "    # Iterate over the dataset and filter images for the specified class\n",
    "    for idx, data in enumerate(dataloader):\n",
    "        images, targets = data\n",
    "        if class_idx in targets.unique().tolist():\n",
    "            count += 1\n",
    "            images_per_class[class_name].append(images)\n",
    "            # Denormalize the image\n",
    "            # img = denormalize(images.squeeze(), mean, std).permute(1, 2, 0).cpu().numpy()\n",
    "            # img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "\n",
    "            # Visualize the image\n",
    "            # plt.imshow(img)\n",
    "            # plt.title(f'Class: {class_name}')\n",
    "            # plt.axis('off')\n",
    "            # plt.show()\n",
    "\n",
    "    return count, images_per_class\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(type='LoadAnnotations'),\n",
    "    dict(type='Resize', img_scale=(512, 512), keep_ratio=True),\n",
    "    dict(type='RandomFlip', flip_ratio=0.5),\n",
    "    dict(type='Normalize', mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True),\n",
    "    dict(type='Pad', size_divisor=32),\n",
    "    dict(type='DefaultFormatBundle'),\n",
    "    dict(type='Collect', keys=['img', 'gt_semantic_seg'])\n",
    "]\n",
    "\n",
    "# Path to the Pascal VOC dataset\n",
    "img_dir = '/Users/micheleverriello/LabelAnything/data/pascal/JPEGImages'\n",
    "ann_dir = '/Users/micheleverriello/LabelAnything/data/pascal/SegmentationClass'\n",
    "split_file = '/Users/micheleverriello/LabelAnything/data/pascal/ImageSets/Segmentation/val.txt'\n",
    "\n",
    "# Load the Pascal VOC dataset with a limit of 10 images\n",
    "dataset = PascalVOCDataset(split=split_file, img_dir=img_dir, ann_dir=ann_dir, pipeline=pipeline, limit=2000)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "count, images_per_class = show_aeroplane_images(dataset, class_name='aeroplane')\n",
    "print(f'Number of images in the \"aeroplane\" class: {count}')\n",
    "\n",
    "# Calculate the mIoU\n",
    "num_classes = len(PascalVOCDataset.CLASSES)\n",
    "miou = evaluate_model_with_support(model, images_per_class, num_classes)\n",
    "print(f'mIoU: {miou}')"
   ],
   "id": "361a08fd59b71e44",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the \"aeroplane\" class: 90\n",
      "Tested on class aeroplane, images 1 to 3\n",
      "Tested on class aeroplane, images 4 to 6\n",
      "Tested on class aeroplane, images 7 to 9\n",
      "Tested on class aeroplane, images 10 to 12\n",
      "Tested on class aeroplane, images 13 to 15\n",
      "Tested on class aeroplane, images 16 to 18\n",
      "Tested on class aeroplane, images 19 to 21\n",
      "Tested on class aeroplane, images 22 to 24\n",
      "Tested on class aeroplane, images 25 to 27\n",
      "Tested on class aeroplane, images 28 to 30\n",
      "Tested on class aeroplane, images 31 to 33\n",
      "Tested on class aeroplane, images 34 to 36\n",
      "Tested on class aeroplane, images 37 to 39\n",
      "Tested on class aeroplane, images 40 to 42\n",
      "Tested on class aeroplane, images 43 to 45\n",
      "Tested on class aeroplane, images 46 to 48\n",
      "Tested on class aeroplane, images 49 to 51\n",
      "Tested on class aeroplane, images 52 to 54\n",
      "Tested on class aeroplane, images 55 to 57\n",
      "Tested on class aeroplane, images 58 to 60\n",
      "Tested on class aeroplane, images 61 to 63\n",
      "Tested on class aeroplane, images 64 to 66\n",
      "Tested on class aeroplane, images 67 to 69\n",
      "Tested on class aeroplane, images 70 to 72\n",
      "Tested on class aeroplane, images 73 to 75\n",
      "Tested on class aeroplane, images 76 to 78\n",
      "Tested on class aeroplane, images 79 to 81\n",
      "Tested on class aeroplane, images 82 to 84\n",
      "Tested on class aeroplane, images 85 to 87\n",
      "Tested on class aeroplane, images 88 to 90\n",
      "mIoU: 0.03324605194600879\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6c9b7cb1828b0b99"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipdino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
