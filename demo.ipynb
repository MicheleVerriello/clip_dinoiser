{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0017a479-b454-4843-9c47-3c4fd0d298b2",
   "metadata": {},
   "source": [
    "# CLIP-DINOiser visualization demo ðŸ–¼ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b88be8f-847e-427d-bdcb-0db278337535",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T11:53:55.978458Z",
     "start_time": "2024-11-06T11:53:55.960776Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/clip_dinoiser/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/miniconda3/envs/clip_dinoiser/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hydra.initialize()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hydra.core.global_hydra import GlobalHydra\n",
    "import os\n",
    "\n",
    "from markdown_it.rules_inline import image\n",
    "from torch import Tensor\n",
    "\n",
    "from models.builder import build_model\n",
    "from helpers.visualization import mask2rgb\n",
    "from segmentation.datasets import PascalVOCDataset\n",
    "from hydra import compose, initialize\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms as T\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from operator import itemgetter \n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "GlobalHydra.instance().clear()\n",
    "initialize(config_path=\"configs\", version_base=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9d3e684",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T11:53:56.068927Z",
     "start_time": "2024-11-06T11:53:56.066727Z"
    }
   },
   "outputs": [],
   "source": [
    "# def load_support_image(image_path):\n",
    "#     \"\"\"Load and preprocess a single support image\"\"\"\n",
    "#     image = Image.open(image_path).convert('RGB')\n",
    "#     transform = T.Compose([\n",
    "#         T.Resize((224, 224)),\n",
    "#         T.ToTensor(),\n",
    "#         T.Normalize((0.48145466, 0.4578275, 0.40821073), \n",
    "#                    (0.26862954, 0.26130258, 0.27577711))\n",
    "#     ])\n",
    "#     return transform(image).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# def load_support_images(support_image_paths):\n",
    "#     \"\"\"Load multiple support images\"\"\"\n",
    "#     support_images = torch.tensor([])\n",
    "#     for path in support_image_paths:\n",
    "#         image_tensor = load_support_image(image_path=path)\n",
    "#         print(image_tensor.shape)\n",
    "#         support_images = torch.cat((support_images, image_tensor), dim=1)\n",
    "    \n",
    "#     print(support_images.shape)\n",
    "#     return support_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070cd90c-cc3e-43a5-99e1-fa3ea666d436",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T11:53:56.073343Z",
     "start_time": "2024-11-06T11:53:56.072124Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0107d95-1244-4d43-8428-ac031fa5a728",
   "metadata": {},
   "source": [
    "### Load and configure a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f904d10-fdf9-47f5-8053-2aeb871f8dba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T11:53:58.016449Z",
     "start_time": "2024-11-06T11:53:56.087893Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CLIP_DINOiser: MaskClip: number of dims don't match in permute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/miniconda3/envs/clip_dinoiser/lib/python3.9/site-packages/mmcv/utils/registry.py:69\u001b[0m, in \u001b[0;36mbuild_from_cfg\u001b[0;34m(cfg, registry, default_args)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# Normal TypeError does not print class name.\u001b[39;00m\n",
      "File \u001b[0;32m~/clip_dinoiser/models/maskclip/maskclip.py:34\u001b[0m, in \u001b[0;36mMaskClip.__init__\u001b[0;34m(self, backbone, decode_head, clip_model, class_names)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28msuper\u001b[39m(MaskClip, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdecode_head\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecode_head\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size \u001b[38;5;241m=\u001b[39m backbone\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/clip_dinoiser/models/maskclip/maskclip.py:146\u001b[0m, in \u001b[0;36mMaskClipHead.__init__\u001b[0;34m(self, clip_model, class_names, in_channels, text_channels, use_templates, pretrained, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_class_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_channels, text_channels, \u001b[38;5;241m1\u001b[39m, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/clip_dinoiser/models/maskclip/maskclip.py:155\u001b[0m, in \u001b[0;36mMaskClipHead._get_class_embeddings\u001b[0;34m(self, text_model, class_names)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_class_embeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_model: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, class_names: \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m--> 155\u001b[0m     aug_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_label(text_model, label) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m class_names])\n\u001b[1;32m    156\u001b[0m     aug_embeddings \u001b[38;5;241m=\u001b[39m aug_embeddings \u001b[38;5;241m/\u001b[39m aug_embeddings\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/clip_dinoiser/models/maskclip/maskclip.py:155\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_class_embeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_model: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, class_names: \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m--> 155\u001b[0m     aug_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m class_names])\n\u001b[1;32m    156\u001b[0m     aug_embeddings \u001b[38;5;241m=\u001b[39m aug_embeddings \u001b[38;5;241m/\u001b[39m aug_embeddings\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/clip_dinoiser/models/maskclip/maskclip.py:163\u001b[0m, in \u001b[0;36mMaskClipHead._embed_label\u001b[0;34m(self, text_model, label)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 163\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mtext_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[0;32m/opt/miniconda3/envs/clip_dinoiser/lib/python3.9/site-packages/open_clip/model.py:275\u001b[0m, in \u001b[0;36mCLIP.encode_text\u001b[0;34m(self, text, normalize)\u001b[0m\n\u001b[1;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding\u001b[38;5;241m.\u001b[39mto(cast_dtype)\n\u001b[0;32m--> 275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# NLD -> LND\u001b[39;00m\n\u001b[1;32m    276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(x, attn_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_mask)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: number of dims don't match in permute",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/miniconda3/envs/clip_dinoiser/lib/python3.9/site-packages/mmcv/utils/registry.py:69\u001b[0m, in \u001b[0;36mbuild_from_cfg\u001b[0;34m(cfg, registry, default_args)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# Normal TypeError does not print class name.\u001b[39;00m\n",
      "File \u001b[0;32m~/clip_dinoiser/models/clip_dinoiser/clip_dinoiser.py:231\u001b[0m, in \u001b[0;36mCLIP_DINOiser.__init__\u001b[0;34m(self, clip_backbone, class_names, vit_arch, vit_patch_size, enc_type_feats, feats_idx, gamma, delta, in_dim, conv_kernel)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, clip_backbone, class_names, vit_arch\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvit_base\u001b[39m\u001b[38;5;124m\"\u001b[39m, vit_patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, enc_type_feats\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    230\u001b[0m              feats_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m, in_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, conv_kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mCLIP_DINOiser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclip_backbone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvit_arch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvit_patch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_type_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_head \u001b[38;5;241m=\u001b[39m MaskClipHead(clip_backbone, class_names, in_channels\u001b[38;5;241m=\u001b[39min_dim, text_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, use_templates\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/clip_dinoiser/models/clip_dinoiser/clip_dinoiser.py:41\u001b[0m, in \u001b[0;36mDinoCLIP.__init__\u001b[0;34m(self, clip_backbone, class_names, vit_arch, vit_patch_size, enc_type_feats, gamma, delta, apply_found)\u001b[0m\n\u001b[1;32m     40\u001b[0m maskclip_cfg \u001b[38;5;241m=\u001b[39m OmegaConf\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigs/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclip_backbone\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_backbone \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaskclip_cfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_backbone\u001b[38;5;241m.\u001b[39mparameters():\n",
      "File \u001b[0;32m~/clip_dinoiser/models/builder.py:8\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(config, class_names)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_model\u001b[39m(config, class_names):\n\u001b[0;32m----> 8\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mMODELS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOmegaConf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_container\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolve\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mdefault_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass_names\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/miniconda3/envs/clip_dinoiser/lib/python3.9/site-packages/mmcv/utils/registry.py:237\u001b[0m, in \u001b[0;36mRegistry.build\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregistry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/clip_dinoiser/lib/python3.9/site-packages/mmcv/utils/registry.py:72\u001b[0m, in \u001b[0;36mbuild_from_cfg\u001b[0;34m(cfg, registry, default_args)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# Normal TypeError does not print class name.\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_cls\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MaskClip: number of dims don't match in permute",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Build the model\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPascalVOCDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLASSES\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Ensure the decode_head is an instance of MaskClipHead\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model\u001b[38;5;241m.\u001b[39mclip_backbone\u001b[38;5;241m.\u001b[39mdecode_head, MaskClipHead):\n",
      "File \u001b[0;32m~/clip_dinoiser/models/builder.py:8\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(config, class_names)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_model\u001b[39m(config, class_names):\n\u001b[0;32m----> 8\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mMODELS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOmegaConf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_container\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolve\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mdefault_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass_names\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/miniconda3/envs/clip_dinoiser/lib/python3.9/site-packages/mmcv/utils/registry.py:237\u001b[0m, in \u001b[0;36mRegistry.build\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregistry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/clip_dinoiser/lib/python3.9/site-packages/mmcv/utils/registry.py:72\u001b[0m, in \u001b[0;36mbuild_from_cfg\u001b[0;34m(cfg, registry, default_args)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj_cls(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# Normal TypeError does not print class name.\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_cls\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CLIP_DINOiser: MaskClip: number of dims don't match in permute"
     ]
    }
   ],
   "source": [
    "from models import MaskClipHead\n",
    "\n",
    "# Load the model checkpoint\n",
    "check_path = './checkpoints/last.pt'\n",
    "check = torch.load(check_path, map_location='cpu')\n",
    "\n",
    "# Load the configuration\n",
    "dinoclip_cfg = \"clip_dinoiser.yaml\"\n",
    "cfg = compose(config_name=dinoclip_cfg)\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Build the model\n",
    "model = build_model(cfg.model, class_names=PascalVOCDataset.CLASSES).to(device)\n",
    "\n",
    "# Ensure the decode_head is an instance of MaskClipHead\n",
    "if not isinstance(model.clip_backbone.decode_head, MaskClipHead):\n",
    "    model.clip_backbone.decode_head = MaskClipHead(\n",
    "        clip_model=model.clip_backbone.decode_head.clip_model,\n",
    "        class_names=PascalVOCDataset.CLASSES,\n",
    "        in_channels=3,\n",
    "        text_channels=512,\n",
    "        use_templates=False,\n",
    "        pretrained='laion2b_s34b_b88k'\n",
    "    ).to(device)\n",
    "\n",
    "# Switching off the imagenet templates for fast inference\n",
    "model.clip_backbone.decode_head.use_templates = False\n",
    "\n",
    "# Load the model state\n",
    "model.load_state_dict(check['model_state_dict'], strict=False)\n",
    "model = model.eval()\n",
    "\n",
    "# Load the Pascal VOC dataset\n",
    "dataset = PascalVOCDataset(\n",
    "    img_dir='path/to/VOCdevkit/VOC2012/JPEGImages',\n",
    "    ann_dir='path/to/VOCdevkit/VOC2012/SegmentationClass',\n",
    "    pipeline=[\n",
    "        dict(type='LoadImageFromFile'),\n",
    "        dict(type='LoadAnnotations'),\n",
    "        dict(type='Resize', img_scale=(512, 512), keep_ratio=True),\n",
    "        dict(type='RandomFlip', flip_ratio=0.5),\n",
    "        dict(type='Normalize', mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True),\n",
    "        dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),\n",
    "        dict(type='DefaultFormatBundle'),\n",
    "        dict(type='Collect', keys=['img', 'gt_semantic_seg']),\n",
    "    ],\n",
    "    split='ImageSets/Segmentation/train.txt'\n",
    ")\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "def select_support_images(data_loader, num_support_images=2):\n",
    "    support_images = []\n",
    "    for i, (img, _) in enumerate(data_loader):\n",
    "        if i >= num_support_images:\n",
    "            break\n",
    "        support_images.append(img)\n",
    "    return torch.cat(support_images, dim=0)\n",
    "\n",
    "def load_image(image_path):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    transform = T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "    ])\n",
    "    return transform(img).unsqueeze(0)\n",
    "\n",
    "\n",
    "# Function to visualize per image\n",
    "def visualize_per_image(file_path, support_image_paths, palette, model):\n",
    "    assert os.path.isfile(file_path), f\"No such file: {file_path}\"\n",
    "\n",
    "    # Load the input image\n",
    "    img = Image.open(file_path).convert('RGB')\n",
    "    img_tens = T.PILToTensor()(img).unsqueeze(0).to(\"cpu\") / 255.\n",
    "\n",
    "    # Select and process support images from the dataset\n",
    "    support_images = load_support_images(support_image_paths).to(\"cpu\")\n",
    "\n",
    "    # Perform inference using the support embeddings\n",
    "    h, w = img_tens.shape[-2:]\n",
    "    output = model(img_tens, support_images=support_images).cpu()\n",
    "    output = F.interpolate(output, scale_factor=model.vit_patch_size, mode=\"bilinear\", align_corners=False)[..., :h, :w]\n",
    "    output = output[0].argmax(dim=0)\n",
    "    mask = mask2rgb(output, palette)\n",
    "\n",
    "    # Visualize the results\n",
    "    fig = plt.figure(figsize=(3, 1))\n",
    "    classes = np.unique(output).tolist()\n",
    "    plt.imshow(np.array(itemgetter(*classes)(palette)).reshape(1, -1, 3))\n",
    "    plt.xticks(np.arange(len(classes)), [f\"Class {i}\" for i in classes], rotation=45)\n",
    "    plt.yticks([])\n",
    "\n",
    "    return mask, fig, img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfa0596-1312-4865-bad8-9b06337513a3",
   "metadata": {},
   "source": [
    "### Example with 'background' class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25ceb49-72c0-4475-baf2-b6a278a1a2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = 'assets/vintage_bike.jpeg'\n",
    "# PALETTE = [(0, 0, 0), (156, 143, 189), (79, 158, 101)]\n",
    "# \n",
    "# # specify your prompts\n",
    "# TEXT_PROMPTS = ['leather bag']\n",
    "# model.clip_backbone.decode_head.update_vocab(TEXT_PROMPTS)\n",
    "# model.to(device)\n",
    "# \n",
    "# # set apply FOUND (background detector) to True\n",
    "# model.apply_found = True\n",
    "# \n",
    "# # run segmentation\n",
    "# mask, ticks, img = visualize_per_image(file, TEXT_PROMPTS, PALETTE, model)\n",
    "# \n",
    "# fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "# alpha=0.5\n",
    "# blend = (alpha)*np.array(img)/255. + (1-alpha) * mask/255.\n",
    "# ax[0].imshow(blend)\n",
    "# ax[1].imshow(mask)\n",
    "# ax[0].axis('off')\n",
    "# ax[1].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0200f3c5-9dd2-4bef-ab41-e3847bf73a3d",
   "metadata": {},
   "source": [
    "### Example without 'background' class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949a1aeb-a14b-4733-a9e9-a3fe29def88f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T11:53:58.020014Z",
     "start_time": "2024-11-06T11:42:08.594661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 1, 3, 224, 224])\n",
      "torch.Size([1, 2, 3, 224, 224])\n",
      "Support images tensor shape: torch.Size([1, 3, 3, 224, 224])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSupport images tensor shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupport_images_tensor\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Specify whether applying FOUND or not\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mapply_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Visualize per image\u001b[39;00m\n\u001b[1;32m     22\u001b[0m mask, ticks, img \u001b[38;5;241m=\u001b[39m visualize_per_image(file, support_image_paths, PALETTE, model)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "file = 'assets/vintage_bike.jpeg'\n",
    "PALETTE = [[25, 29, 136], [128, 112, 112], [85, 124, 85], [250, 112, 112], [250, 250, 0], [250, 0, 0]]\n",
    "num_support_images = 2  # Number of support images to select from the dataset\n",
    "\n",
    "# Add a cell to input the paths to support images\n",
    "support_image_paths = [\n",
    "    \"assets/bike.jpeg\",\n",
    "    \"assets/bag.jpeg\",\n",
    "]\n",
    "\n",
    "# # Load the support images\n",
    "# support_images_tensor = load_support_images(support_image_paths)\n",
    "\n",
    "# # add the query image\n",
    "# support_images_tensor = torch.cat((support_images_tensor, load_support_image(file).to(\"cpu\")), dim=1)\n",
    "# print(f'Support images tensor shape: {support_images_tensor.shape}')\n",
    "\n",
    "# Specify whether applying FOUND or not\n",
    "model.apply_found = True\n",
    "\n",
    "mask, fig, img = visualize_per_image(file_path, num_support_images, palette, model)\n",
    "\n",
    "\n",
    "# Display the results\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "alpha = 0.5\n",
    "blend = (alpha) * np.array(img) / 255. + (1 - alpha) * mask / 255.\n",
    "ax[0].imshow(blend)\n",
    "ax[1].imshow(mask)\n",
    "ax[0].axis('off')\n",
    "ax[1].axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip_dinoiser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
