{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0017a479-b454-4843-9c47-3c4fd0d298b2",
   "metadata": {},
   "source": [
    "# CLIP-DINOiser visualization demo üñºÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "id": "cc27d12a1279e4cc",
   "metadata": {},
   "source": [
    "from hydra.core.global_hydra import GlobalHydra\n",
    "import os\n",
    "from models.builder import build_model\n",
    "from helpers.visualization import mask2rgb\n",
    "from segmentation.datasets import PascalVOCDataset\n",
    "from hydra import compose, initialize\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms as T\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "GlobalHydra.instance().clear()\n",
    "initialize(config_path=\"configs\", version_base=None)\n",
    "\n",
    "def visualize_per_image(file_path, support_files, palette, model, class_names):\n",
    "    # Assert that the main image exists\n",
    "    assert os.path.isfile(file_path), f\"No such file: {file_path}\"\n",
    "    \n",
    "    # print(f\"Dataset classes: {dataset_classes}\")\n",
    "\n",
    "    # Open and preprocess the main image\n",
    "    img = Image.open(file_path).convert('RGB')\n",
    "    img_tens = T.PILToTensor()(img).unsqueeze(0).to(device) / 255.\n",
    "\n",
    "    # Load and preprocess support images\n",
    "    support_images = []\n",
    "    for support_file in support_files:\n",
    "        assert os.path.isfile(support_file), f\"No such file: {support_file}\"\n",
    "        support_img = Image.open(support_file).convert('RGB')\n",
    "\n",
    "        # Resize the support images to match the size of the main image\n",
    "        support_img_resized = support_img.resize(img.size, Image.BILINEAR)\n",
    "\n",
    "        # Convert to tensor and normalize\n",
    "        support_img_tens = T.PILToTensor()(support_img_resized).unsqueeze(0).to(device) / 255.\n",
    "        support_images.append(support_img_tens)\n",
    "    \n",
    "    # Stack the support images into a batch\n",
    "    support_images = torch.cat(support_images, dim=0).to(device)  \n",
    "    # for spt in support_images:\n",
    "    #     print(spt.shape)\n",
    "    \n",
    "    # Get the original height and width of the image\n",
    "    h, w = img_tens.shape[-2:]\n",
    "    merged = torch.cat((img_tens, support_images), dim=0)\n",
    "    # print(f'merged: {merged.shape}')\n",
    "    \n",
    "    # Run the model for segmentation using both the main image and the support images\n",
    "    output = model(merged).cpu()  # Ensure your model can accept both inputs\n",
    "    output = F.interpolate(output, scale_factor=model.vit_patch_size, mode=\"bilinear\", align_corners=False)[..., :h, :w]\n",
    "    \n",
    "    # Visualizza le probabilit√† per ogni classe\n",
    "    # output[0] contiene le probabilit√† per ciascun pixel e ciascuna classe\n",
    "    # output[0].shape sar√† [C, H, W], dove C √® il numero di classi\n",
    "    \n",
    "    # Itera su tutte le classi per stampare le probabilit√†\n",
    "    # C, H, W = output[0].shape\n",
    "    # for class_idx in range(C):\n",
    "    #     class_probs = output[0][class_idx]\n",
    "    #     print(f\"Classe {class_idx}:\")\n",
    "    #     print(f\"Probabilit√† media per classe {class_idx}: {class_probs.mean().item()}\")\n",
    "    #     print(\"-\" * 50)\n",
    "    \n",
    "    output = output[0].argmax(dim=0)  # Get the most likely class for each pixel\n",
    "    \n",
    "    # Convert the output to an RGB mask using the provided palette\n",
    "    mask = mask2rgb(output, palette)\n",
    "\n",
    "    # Extract unique class indices from the output mask and map to the palette\n",
    "    detected_classes = np.unique(output).tolist()\n",
    "    # print(f\"Detected classes (indices): {detected_classes}\")\n",
    "\n",
    "    # Filter out the classes that were not detected and print their names\n",
    "    detected_class_names = [class_names[idx] for idx in detected_classes if idx < len(class_names)]\n",
    "    # print(f\"Detected class names: {detected_class_names}\")\n",
    "\n",
    "    # Create the visualization: segmented mask and original image\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "    \n",
    "    # Blending the original image with the mask for visualization\n",
    "    alpha = 0.5\n",
    "    blend = (alpha) * np.array(img) / 255. + (1 - alpha) * mask / 255.\n",
    "    ax[0].imshow(blend)\n",
    "    ax[0].axis('off')\n",
    "    \n",
    "    ax[1].imshow(mask)\n",
    "    ax[1].axis('off')\n",
    "    \n",
    "    # Visualization of class colors along with support image file names as labels\n",
    "    class_colors = np.array([palette[class_idx] for class_idx in detected_classes if class_idx < len(palette)])\n",
    "    plt.figure(figsize=(6, 1))\n",
    "    plt.imshow(class_colors.reshape(1, -1, 3))\n",
    "    plt.xticks(np.arange(len(detected_class_names)), detected_class_names, rotation=45)\n",
    "    plt.yticks([])\n",
    "\n",
    "    return mask, fig, img\n",
    "\n",
    "check_path = './checkpoints/last.pt'\n",
    "check = torch.load(check_path, map_location='cpu')\n",
    "dinoclip_cfg = \"clip_dinoiser.yaml\"\n",
    "cfg = compose(config_name=dinoclip_cfg)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = build_model(cfg.model, class_names=PascalVOCDataset.CLASSES).to(device)\n",
    "model.clip_backbone.decode_head.use_templates = False  # switching off the imagenet templates for fast inference\n",
    "model.load_state_dict(check['model_state_dict'], strict=False)\n",
    "model = model.eval()\n",
    "# TEST WITH TWO SUPPORT IMAGES\n",
    "file = 'assets/airplane.jpg'\n",
    "support_files = ['assets/air2.jpg']  # Two support images\n",
    "\n",
    "PALETTE = [(0, 0, 0), (156, 143, 189), (79, 158, 101)]\n",
    "\n",
    "# Run segmentation with two support images, no text prompts required\n",
    "model.apply_found = True  # assuming this flag is still relevant for your setup\n",
    "\n",
    "# Lista dei nomi delle classi (questo esempio √® per 3 classi)\n",
    "class_names = ['background', 'aeroplane', 'bicycle']\n",
    "# Run segmentation\n",
    "mask, ticks, img = visualize_per_image(file, support_files, PALETTE, model, class_names)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation",
   "id": "7e7a452390628c17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T13:34:13.121337Z",
     "start_time": "2024-11-08T13:33:32.705689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from datasets_custom import PascalVOCDataset # Import the PascalVOCDataset class\n",
    "\n",
    "\n",
    "dinoclip_cfg = \"clip_dinoiser.yaml\"\n",
    "cfg = compose(config_name=dinoclip_cfg)\n",
    "\n",
    "def calculate_confusion_matrix(pred, target, num_classes):\n",
    "    \"\"\"\n",
    "    Calculate the confusion matrix for a single batch.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): Predicted segmentation map.\n",
    "        target (Tensor): Ground truth segmentation map.\n",
    "        num_classes (int): Number of classes.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Confusion matrix for the batch.\n",
    "    \"\"\"\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    mask = (target >= 0) & (target < num_classes)\n",
    "    hist = np.bincount(\n",
    "        num_classes * target[mask].cpu().numpy() + pred[mask].cpu().numpy(),\n",
    "        minlength=num_classes ** 2\n",
    "    ).reshape(num_classes, num_classes)\n",
    "    return hist\n",
    "\n",
    "def evaluate_model(model, dataloader, num_classes):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the given dataloader and calculate the mean IoU.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to evaluate.\n",
    "        dataloader (DataLoader): DataLoader for the dataset.\n",
    "        num_classes (int): Number of classes.\n",
    "\n",
    "    Returns:\n",
    "        float: Mean IoU across all images.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    confusion_matrix = np.zeros((num_classes, num_classes))\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(dataloader):\n",
    "            images, targets = data\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(images)\n",
    "            outputs = F.interpolate(outputs, size=targets.shape[-2:], mode='bilinear', align_corners=False)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            confusion_matrix += calculate_confusion_matrix(preds, targets, num_classes)\n",
    "            print(f'Processed image {idx + 1}')\n",
    "    \n",
    "    # Calculate mIoU from the confusion matrix\n",
    "    intersection = np.diag(confusion_matrix)\n",
    "    union = np.sum(confusion_matrix, axis=0) + np.sum(confusion_matrix, axis=1) - intersection\n",
    "    iou = intersection / np.maximum(union, 1)\n",
    "    miou = np.nanmean(iou)\n",
    "    return miou\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(type='LoadAnnotations'),\n",
    "    dict(type='Resize', img_scale=(512, 512), keep_ratio=True),\n",
    "    dict(type='RandomFlip', flip_ratio=0.5),\n",
    "    dict(type='Normalize', mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True),\n",
    "    dict(type='Pad', size_divisor=32),\n",
    "    dict(type='DefaultFormatBundle'),\n",
    "    dict(type='Collect', keys=['img', 'gt_semantic_seg'])\n",
    "]\n",
    "\n",
    "# Path to the Pascal VOC dataset\n",
    "img_dir = '/Users/micheleverriello/LabelAnything/data/pascal/JPEGImages'\n",
    "ann_dir = '/Users/micheleverriello/LabelAnything/data/pascal/SegmentationClass'\n",
    "split_file = '/Users/micheleverriello/LabelAnything/data/pascal/ImageSets/Segmentation/val.txt'\n",
    "\n",
    "# Load the Pascal VOC dataset with a limit of 10 images\n",
    "dataset = PascalVOCDataset(split=split_file, img_dir=img_dir, ann_dir=ann_dir, pipeline=pipeline, limit=10)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "# Calculate the mIoU\n",
    "num_classes = len(PascalVOCDataset.CLASSES)\n",
    "miou = evaluate_model(model, dataloader, num_classes)\n",
    "print(f'mIoU: {miou}')"
   ],
   "id": "361a08fd59b71e44",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed image 1\n",
      "Processed image 2\n",
      "Processed image 3\n",
      "Processed image 4\n",
      "Processed image 5\n",
      "Processed image 6\n",
      "Processed image 7\n",
      "Processed image 8\n",
      "Processed image 9\n",
      "Processed image 10\n",
      "mIoU: 0.09318704341895108\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipdino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
