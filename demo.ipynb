{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0017a479-b454-4843-9c47-3c4fd0d298b2",
   "metadata": {},
   "source": [
    "# CLIP-DINOiser visualization demo üñºÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "id": "cc27d12a1279e4cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T19:39:25.895637Z",
     "start_time": "2024-11-06T19:39:24.498102Z"
    }
   },
   "source": [
    "from hydra.core.global_hydra import GlobalHydra\n",
    "import os\n",
    "from models.builder import build_model\n",
    "from helpers.visualization import mask2rgb\n",
    "from segmentation.datasets import PascalVOCDataset\n",
    "from hydra import compose, initialize\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms as T\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from operator import itemgetter \n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "GlobalHydra.instance().clear()\n",
    "initialize(config_path=\"configs\", version_base=None)\n",
    "# \n",
    "# def visualize_per_image(file_path, text_prompts, palette, model):\n",
    "#     assert os.path.isfile(file_path), f\"No such file: {file_path}\"\n",
    "# \n",
    "#     img = Image.open(file_path).convert('RGB')\n",
    "#     img_tens = T.PILToTensor()(img).unsqueeze(0).to(device) / 255.\n",
    "# \n",
    "#     h, w = img_tens.shape[-2:]\n",
    "#     output = model(img_tens).cpu()\n",
    "#     output = F.interpolate(output, scale_factor=model.vit_patch_size, mode=\"bilinear\", align_corners=False)[..., :h, :w]\n",
    "#     output = output[0].argmax(dim=0)\n",
    "#     mask = mask2rgb(output, palette)\n",
    "# \n",
    "#     fig = plt.figure(figsize=(3, 1))\n",
    "#     classes = np.unique(output).tolist()\n",
    "#     plt.imshow(np.array(itemgetter(*classes)(PALETTE)).reshape(1, -1, 3))\n",
    "#     plt.xticks(np.arange(len(classes)), list(itemgetter(*classes)(text_prompts)), rotation=45)\n",
    "#     plt.yticks([])\n",
    "# \n",
    "#     return mask, fig, img\n",
    "# check_path = './checkpoints/last.pt'\n",
    "# check = torch.load(check_path, map_location='cpu')\n",
    "# dinoclip_cfg = \"clip_dinoiser.yaml\"\n",
    "# cfg = compose(config_name=dinoclip_cfg)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f'PascalVOCDataset.CLASSES: {PascalVOCDataset.CLASSES}')\n",
    "# model = build_model(cfg.model, class_names=PascalVOCDataset.CLASSES).to(device)\n",
    "# model.clip_backbone.decode_head.use_templates=False # switching off the imagenet templates for fast inference\n",
    "# model.load_state_dict(check['model_state_dict'], strict=False)\n",
    "# model = model.eval()\n",
    "# \n",
    "# # TEST WITH BACKGROUND DETECTOR\n",
    "# file = 'assets/vintage_bike.jpeg'\n",
    "# PALETTE = [(0, 0, 0), (156, 143, 189), (79, 158, 101)]\n",
    "# \n",
    "# # specify your prompts\n",
    "# TEXT_PROMPTS = ['background', 'vintage bike', 'leather bag']\n",
    "# model.clip_backbone.decode_head.update_vocab(TEXT_PROMPTS)\n",
    "# model.to(device)\n",
    "# \n",
    "# # set apply FOUND (background detector) to True\n",
    "# model.apply_found = True\n",
    "# \n",
    "# # run segmentation\n",
    "# mask, ticks, img = visualize_per_image(file, TEXT_PROMPTS, PALETTE, model)\n",
    "# \n",
    "# fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "# alpha=0.5\n",
    "# blend = (alpha)*np.array(img)/255. + (1-alpha) * mask/255.\n",
    "# ax[0].imshow(blend)\n",
    "# ax[1].imshow(mask)\n",
    "# ax[0].axis('off')\n",
    "# ax[1].axis('off')\n",
    "# \n",
    "# # TEST WITHOUT BACKGROUND DETECTOR\n",
    "# \n",
    "# file = 'assets/rusted_van.png'\n",
    "# \n",
    "# PALETTE = [[25, 29, 136], [128, 112, 112], [85, 124, 85], [250, 112, 112], [250, 250, 0], [250, 0, 0]]\n",
    "# \n",
    "# # specify TEXT PROMPTS\n",
    "# TEXT_PROMPTS = [\"rusted van\", \"green trees\", \"foggy clouds\", \"mountains\"] \n",
    "# model.clip_backbone.decode_head.update_vocab(TEXT_PROMPTS)\n",
    "# model.to(device)\n",
    "# \n",
    "# # specify whether applying FOUND or not\n",
    "# model.apply_found = False\n",
    "# mask, ticks, img = visualize_per_image(file, TEXT_PROMPTS, PALETTE, model)\n",
    "# \n",
    "# fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "# alpha=0.5\n",
    "# blend = (alpha)*np.array(img)/255. + (1-alpha)*mask/255.\n",
    "# ax[0].imshow(blend)\n",
    "# ax[1].imshow(mask)\n",
    "# ax[0].axis('off')\n",
    "# ax[1].axis('off')\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/clip_dinoiser/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/miniconda3/envs/clip_dinoiser/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hydra.initialize()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "e4871c1c7ed0aea3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T19:39:25.903657Z",
     "start_time": "2024-11-06T19:39:25.898619Z"
    }
   },
   "source": [
    "def visualize_per_image(file_path, support_files, palette, model):\n",
    "    # Assert that the main image exists\n",
    "    assert os.path.isfile(file_path), f\"No such file: {file_path}\"\n",
    "\n",
    "    # Open and preprocess the main image\n",
    "    img = Image.open(file_path).convert('RGB')\n",
    "    img_tens = T.PILToTensor()(img).unsqueeze(0).to(device) / 255.\n",
    "\n",
    "    # Load and preprocess support images\n",
    "    support_images = []\n",
    "    for support_file in support_files:\n",
    "        assert os.path.isfile(support_file), f\"No such file: {support_file}\"\n",
    "        support_img = Image.open(support_file).convert('RGB')\n",
    "\n",
    "        # Resize the support images to match the size of the main image\n",
    "        support_img_resized = support_img.resize(img.size, Image.BILINEAR)\n",
    "\n",
    "        # Convert to tensor and normalize\n",
    "        support_img_tens = T.PILToTensor()(support_img_resized).unsqueeze(0).to(device) / 255.\n",
    "        support_images.append(support_img_tens)\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Stack the support images into a batch\n",
    "    support_images = torch.cat(support_images, dim=0).to(device)  \n",
    "    for spt in support_images:\n",
    "        print(spt.shape)\n",
    "    # Get the original height and width of the image\n",
    "    h, w = img_tens.shape[-2:]\n",
    "    merged = torch.cat((img_tens, support_images), dim=0)\n",
    "    print(f'merged: {merged.shape}')\n",
    "    # Run the model for segmentation using both the main image and the support images\n",
    "    output = model(merged).cpu()  # Ensure your model can accept both inputs\n",
    "    output = F.interpolate(output, scale_factor=model.vit_patch_size, mode=\"bilinear\", align_corners=False)[..., :h, :w]\n",
    "    output = output[0].argmax(dim=0)  # Get the most likely class for each pixel\n",
    "    \n",
    "    # Convert the output to an RGB mask using the provided palette\n",
    "    mask = mask2rgb(output, palette)\n",
    "\n",
    "    # Create the visualization: segmented mask and original image\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "    \n",
    "    # Blending the original image with the mask for visualization\n",
    "    alpha = 0.5\n",
    "    blend = (alpha) * np.array(img) / 255. + (1 - alpha) * mask / 255.\n",
    "    ax[0].imshow(blend)\n",
    "    ax[0].axis('off')\n",
    "    \n",
    "    ax[1].imshow(mask)\n",
    "    ax[1].axis('off')\n",
    "    \n",
    "    # Extract unique class indices from the output mask and map to the palette\n",
    "    classes = np.unique(output).tolist()\n",
    "    class_colors = np.array([palette[class_idx] for class_idx in classes])\n",
    "\n",
    "    # Visualization of class colors along with support image file names as labels\n",
    "    plt.figure(figsize=(6, 1))\n",
    "    plt.imshow(class_colors.reshape(1, -1, 3))\n",
    "    plt.xticks(np.arange(len(classes)), [f\"Class {i}\" for i in classes], rotation=45)\n",
    "    plt.yticks([])\n",
    "\n",
    "    return mask, fig, img\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "c4625c508a4575af",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-06T19:39:25.935510Z"
    }
   },
   "source": [
    "check_path = './checkpoints/last.pt'\n",
    "check = torch.load(check_path, map_location='cpu')\n",
    "dinoclip_cfg = \"clip_dinoiser.yaml\"\n",
    "cfg = compose(config_name=dinoclip_cfg)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = build_model(cfg.model, class_names=PascalVOCDataset.CLASSES).to(device)\n",
    "model.clip_backbone.decode_head.use_templates = False  # switching off the imagenet templates for fast inference\n",
    "model.load_state_dict(check['model_state_dict'], strict=False)\n",
    "model = model.eval()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "949a1aeb-a14b-4733-a9e9-a3fe29def88f",
   "metadata": {},
   "source": [
    "# TEST WITH TWO SUPPORT IMAGES\n",
    "file = 'assets/vintage_bike.jpeg'\n",
    "support_files = ['assets/bike.jpeg', 'assets/bag.jpeg']  # Two support images\n",
    "\n",
    "PALETTE = [(0, 0, 0), (156, 143, 189), (79, 158, 101)]\n",
    "\n",
    "# Run segmentation with two support images, no text prompts required\n",
    "model.apply_found = True  # assuming this flag is still relevant for your setup\n",
    "\n",
    "# Run segmentation\n",
    "mask, ticks, img = visualize_per_image(file, support_files, PALETTE, model)\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "alpha = 0.5\n",
    "blend = (alpha) * np.array(img) / 255. + (1 - alpha) * mask / 255.\n",
    "ax[0].imshow(blend)\n",
    "ax[1].imshow(mask)\n",
    "ax[0].axis('off')\n",
    "ax[1].axis('off')"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipdino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
